import type { TokenCredential } from "@azure/core-auth";
import type { PagedAsyncIterableIterator } from "@azure/core-paging";
import type { Pipeline, StoragePipelineOptions } from "./Pipeline.js";
import { StorageSharedKeyCredential } from "./credentials/StorageSharedKeyCredential.js";
import { AnonymousCredential } from "@azure/storage-blob";
import { DataLakeLeaseClient } from "./DataLakeLeaseClient.js";
import type { AccessPolicy, FileSystemCreateOptions, FileSystemCreateResponse, FileSystemDeleteOptions, FileSystemDeleteResponse, FileSystemExistsOptions, FileSystemGetAccessPolicyOptions, FileSystemGetAccessPolicyResponse, FileSystemGetPropertiesOptions, FileSystemGetPropertiesResponse, FileSystemSetAccessPolicyOptions, FileSystemSetAccessPolicyResponse, FileSystemSetMetadataOptions, FileSystemSetMetadataResponse, ListPathsOptions, Metadata, Path, PublicAccessType, SignedIdentifier, FileSystemListPathsResponse, FileSystemCreateIfNotExistsResponse, FileSystemDeleteIfExistsResponse, FileSystemGenerateSasUrlOptions, FileSystemListDeletedPathsResponse, ListDeletedPathsOptions, DeletedPath, FileSystemUndeletePathResponse, FileSystemUndeletePathOption, UserDelegationKey } from "./models.js";
import { StorageClient } from "./StorageClient.js";
import { DataLakeFileClient, DataLakeDirectoryClient } from "./clients.js";
/**
 * A DataLakeFileSystemClient represents a URL to the Azure Storage file system
 * allowing you to manipulate its directories and files.
 */
export declare class DataLakeFileSystemClient extends StorageClient {
    /**
     * fileSystemContext provided by protocol layer.
     */
    private fileSystemContext;
    /**
     * fileSystemContext provided by protocol layer.
     */
    private fileSystemContextToBlobEndpoint;
    /**
     * blobContainerClient provided by `@azure/storage-blob` package.
     */
    private blobContainerClient;
    /**
     * Creates an instance of DataLakeFileSystemClient from url and credential.
     *
     * @param url - A Client string pointing to Azure Storage data lake file system, such as
     *                     "https://myaccount.dfs.core.windows.net/filesystem". You can append a SAS
     *                     if using AnonymousCredential, such as "https://myaccount.dfs.core.windows.net/filesystem?sasString".
     * @param credential - Such as AnonymousCredential, StorageSharedKeyCredential or any credential from the `@azure/identity` package to authenticate requests to the service. You can also provide an object that implements the TokenCredential interface. If not specified, AnonymousCredential is used.
     * @param options - Optional. Options to configure the HTTP pipeline.
     */
    constructor(url: string, credential?: StorageSharedKeyCredential | AnonymousCredential | TokenCredential, options?: StoragePipelineOptions);
    /**
     * Creates an instance of DataLakeFileSystemClient from url and pipeline.
     *
     * @param url - A Client string pointing to Azure Storage data lake file system, such as
     *                     "https://myaccount.dfs.core.windows.net/filesystem". You can append a SAS
     *                     if using AnonymousCredential, such as "https://myaccount.dfs.core.windows.net/filesystem?sasString".
     * @param pipeline - Call newPipeline() to create a default
     *                            pipeline, or provide a customized pipeline.
     */
    constructor(url: string, pipeline: Pipeline);
    /**
     * Name of current file system.
     *
     * @readonly
     */
    get name(): string;
    /**
     * Creates a {@link DataLakeDirectoryClient} object under current file system.
     *
     * @param directoryName -
     */
    getDirectoryClient(directoryName: string): DataLakeDirectoryClient;
    /**
     * Creates a {@link DataLakeFileClient} object under current file system.
     *
     * @param fileName -
     */
    getFileClient(fileName: string): DataLakeFileClient;
    /**
     * Get a {@link DataLakeLeaseClient} that manages leases on the file system.
     *
     * @param proposeLeaseId - Optional. Initial proposed lease Id.
     */
    getDataLakeLeaseClient(proposeLeaseId?: string): DataLakeLeaseClient;
    /**
     * Creates a new file system under the specified account. If the file system with
     * the same name already exists, the operation fails.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/create-container
     *
     * @param options - Optional. Options when creating file system.
     */
    create(options?: FileSystemCreateOptions): Promise<FileSystemCreateResponse>;
    /**
     * Creates a new file system under the specified account. If the file system with
     * the same name already exists, it is not changed.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/create-container
     *
     * @param options -
     */
    createIfNotExists(options?: FileSystemCreateOptions): Promise<FileSystemCreateIfNotExistsResponse>;
    /**
     * Returns true if the File system represented by this client exists; false otherwise.
     *
     * NOTE: use this function with care since an existing file system might be deleted by other clients or
     * applications. Vice versa new file system with the same name might be added by other clients or
     * applications after this function completes.
     *
     * @param options -
     */
    exists(options?: FileSystemExistsOptions): Promise<boolean>;
    /**
     * Delete current file system.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/delete-container
     *
     * @param options - Optional. Options when deleting file system.
     */
    delete(options?: FileSystemDeleteOptions): Promise<FileSystemDeleteResponse>;
    /**
     * Delete current file system if it exists.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/delete-container
     *
     * @param options -
     */
    deleteIfExists(options?: FileSystemDeleteOptions): Promise<FileSystemDeleteIfExistsResponse>;
    /**
     * Returns all user-defined metadata and system properties for the specified
     * file system.
     *
     * WARNING: The `metadata` object returned in the response will have its keys in lowercase, even if
     * they originally contained uppercase characters. This differs from the metadata keys returned by
     * the `listFileSystems` method of {@link DataLakeServiceClient} using the `includeMetadata` option, which
     * will retain their original casing.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/get-container-properties
     *
     * @param options - Optional. Options when getting file system properties.
     */
    getProperties(options?: FileSystemGetPropertiesOptions): Promise<FileSystemGetPropertiesResponse>;
    /**
     * Sets one or more user-defined name-value pairs for the specified file system.
     *
     * If no option provided, or no metadata defined in the parameter, the file system
     * metadata will be removed.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/set-container-metadata
     *
     * @param metadata - Replace existing metadata with this value.
     *                              If no value provided the existing metadata will be removed.
     * @param options - Optional. Options when setting file system metadata.
     */
    setMetadata(metadata?: Metadata, options?: FileSystemSetMetadataOptions): Promise<FileSystemSetMetadataResponse>;
    /**
     * Gets the permissions for the specified file system. The permissions indicate
     * whether file system data may be accessed publicly.
     *
     * WARNING: JavaScript Date will potentially lose precision when parsing startsOn and expiresOn strings.
     * For example, new Date("2018-12-31T03:44:23.8827891Z").toISOString() will get "2018-12-31T03:44:23.882Z".
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/get-container-acl
     *
     * @param options - Optional. Options when getting file system access policy.
     */
    getAccessPolicy(options?: FileSystemGetAccessPolicyOptions): Promise<FileSystemGetAccessPolicyResponse>;
    /**
     * Sets the permissions for the specified file system. The permissions indicate
     * whether directories or files in a file system may be accessed publicly.
     *
     * When you set permissions for a file system, the existing permissions are replaced.
     * If no access or containerAcl provided, the existing file system ACL will be
     * removed.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/set-container-acl
     *
     * @param access - Optional. The level of public access to data in the file system.
     * @param fileSystemAcl - Optional. Array of elements each having a unique Id and details of the access policy.
     * @param options - Optional. Options when setting file system access policy.
     */
    setAccessPolicy(access?: PublicAccessType, fileSystemAcl?: SignedIdentifier<AccessPolicy>[], options?: FileSystemSetAccessPolicyOptions): Promise<FileSystemSetAccessPolicyResponse>;
    /**
     * Returns an async iterable iterator to list all the paths (directories and files)
     * under the specified file system.
     *
     * .byPage() returns an async iterable iterator to list the paths in pages.
     *
     * Example using `for await` syntax:
     *
     * ```ts snippet:ReadmeSampleListPaths
     * import { DataLakeServiceClient } from "@azure/storage-file-datalake";
     * import { DefaultAzureCredential } from "@azure/identity";
     *
     * const account = "<account>";
     * const datalakeServiceClient = new DataLakeServiceClient(
     *   `https://${account}.dfs.core.windows.net`,
     *   new DefaultAzureCredential(),
     * );
     *
     * const fileSystemName = "<file system name>";
     * const fileSystemClient = datalakeServiceClient.getFileSystemClient(fileSystemName);
     *
     * let i = 1;
     * const paths = fileSystemClient.listPaths();
     * for await (const path of paths) {
     *   console.log(`Path ${i++}: ${path.name}, is directory: ${path.isDirectory}`);
     * }
     * ```
     *
     * Example using `iter.next()`:
     *
     * ```ts snippet:ReadmeSampleListPaths_Iterator
     * import { DataLakeServiceClient } from "@azure/storage-file-datalake";
     * import { DefaultAzureCredential } from "@azure/identity";
     *
     * const account = "<account>";
     * const datalakeServiceClient = new DataLakeServiceClient(
     *   `https://${account}.dfs.core.windows.net`,
     *   new DefaultAzureCredential(),
     * );
     *
     * const fileSystemName = "<file system name>";
     * const fileSystemClient = datalakeServiceClient.getFileSystemClient(fileSystemName);
     *
     * let i = 1;
     * const paths = fileSystemClient.listPaths();
     * let { value, done } = await paths.next();
     * while (!done) {
     *   console.log(`Path ${i++}: ${value.name}, is directory: ${value.isDirectory}`);
     *   ({ value, done } = await paths.next());
     * }
     * ```
     *
     * Example using `byPage()`:
     *
     * ```ts snippet:ReadmeSampleListPaths_ByPage
     * import { DataLakeServiceClient } from "@azure/storage-file-datalake";
     * import { DefaultAzureCredential } from "@azure/identity";
     *
     * const account = "<account>";
     * const datalakeServiceClient = new DataLakeServiceClient(
     *   `https://${account}.dfs.core.windows.net`,
     *   new DefaultAzureCredential(),
     * );
     *
     * const fileSystemName = "<file system name>";
     * const fileSystemClient = datalakeServiceClient.getFileSystemClient(fileSystemName);
     *
     * let i = 1;
     * for await (const response of fileSystemClient.listPaths().byPage({ maxPageSize: 20 })) {
     *   if (response.pathItems) {
     *     for (const path of response.pathItems) {
     *       console.log(`Path ${i++}: ${path.name}, is directory: ${path.isDirectory}`);
     *     }
     *   }
     * }
     * ```
     *
     * Example using paging with a marker:
     *
     * ```ts snippet:ReadmeSampleListPaths_Continuation
     * import { DataLakeServiceClient } from "@azure/storage-file-datalake";
     * import { DefaultAzureCredential } from "@azure/identity";
     *
     * const account = "<account>";
     * const datalakeServiceClient = new DataLakeServiceClient(
     *   `https://${account}.dfs.core.windows.net`,
     *   new DefaultAzureCredential(),
     * );
     *
     * const fileSystemName = "<file system name>";
     * const fileSystemClient = datalakeServiceClient.getFileSystemClient(fileSystemName);
     *
     * let i = 1;
     * let paths = fileSystemClient.listPaths().byPage({ maxPageSize: 2 });
     * let response = (await paths.next()).value;
     * // Prints 2 paths
     * if (response.pathItems) {
     *   for (const path of response.pathItems) {
     *     console.log(`Path ${i++}: ${path.name}, is directory: ${path.isDirectory}`);
     *   }
     * }
     * // Gets next marker
     * let marker = response.continuationToken;
     * // Passing next marker as continuationToken
     * paths = fileSystemClient.listPaths().byPage({ continuationToken: marker, maxPageSize: 10 });
     * response = (await paths.next()).value;
     * // Prints 10 paths
     * if (response.pathItems) {
     *   for (const path of response.pathItems) {
     *     console.log(`Path ${i++}: ${path.name}, is directory: ${path.isDirectory}`);
     *   }
     * }
     * ```
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/list-blobs
     *
     * @param options - Optional. Options when listing paths.
     */
    listPaths(options?: ListPathsOptions): PagedAsyncIterableIterator<Path, FileSystemListPathsResponse>;
    private listItems;
    private listSegments;
    private listPathsSegment;
    /**
     * Returns an async iterable iterator to list all the paths (directories and files)
     * under the specified file system.
     *
     * .byPage() returns an async iterable iterator to list the paths in pages.
     *
     * Example using `for await` syntax:
     *
     * ```ts snippet:ReadmeSampleListDeletedPaths
     * import { DataLakeServiceClient } from "@azure/storage-file-datalake";
     * import { DefaultAzureCredential } from "@azure/identity";
     *
     * const account = "<account>";
     * const datalakeServiceClient = new DataLakeServiceClient(
     *   `https://${account}.dfs.core.windows.net`,
     *   new DefaultAzureCredential(),
     * );
     *
     * const fileSystemName = "<file system name>";
     * const fileSystemClient = datalakeServiceClient.getFileSystemClient(fileSystemName);
     *
     * let i = 1;
     * const deletedPaths = fileSystemClient.listDeletedPaths();
     * for await (const deletedPath of deletedPaths) {
     *   console.log(`Deleted path ${i++}: ${deletedPath.name}, deleted on: ${deletedPath.deletedOn}`);
     * }
     * ```
     *
     * Example using `iter.next()`:
     *
     * ```ts snippet:ReadmeSampleListDeletedPaths_Iterator
     * import { DataLakeServiceClient } from "@azure/storage-file-datalake";
     * import { DefaultAzureCredential } from "@azure/identity";
     *
     * const account = "<account>";
     * const datalakeServiceClient = new DataLakeServiceClient(
     *   `https://${account}.dfs.core.windows.net`,
     *   new DefaultAzureCredential(),
     * );
     *
     * const fileSystemName = "<file system name>";
     * const fileSystemClient = datalakeServiceClient.getFileSystemClient(fileSystemName);
     *
     * let i = 1;
     * const deletedPaths = fileSystemClient.listDeletedPaths();
     * let { value, done } = await deletedPaths.next();
     * while (!done) {
     *   console.log(`Deleted path ${i++}: ${value.name}, deleted on: ${value.deletedOn}`);
     *   ({ value, done } = await deletedPaths.next());
     * }
     * ```
     *
     * Example using `byPage()`:
     *
     * ```ts snippet:ReadmeSampleListDeletedPaths_ByPage
     * import { DataLakeServiceClient } from "@azure/storage-file-datalake";
     * import { DefaultAzureCredential } from "@azure/identity";
     *
     * const account = "<account>";
     * const datalakeServiceClient = new DataLakeServiceClient(
     *   `https://${account}.dfs.core.windows.net`,
     *   new DefaultAzureCredential(),
     * );
     *
     * const fileSystemName = "<file system name>";
     * const fileSystemClient = datalakeServiceClient.getFileSystemClient(fileSystemName);
     *
     * let i = 1;
     * for await (const response of fileSystemClient.listDeletedPaths().byPage({ maxPageSize: 20 })) {
     *   if (response.pathItems) {
     *     for (const deletedPath of response.pathItems) {
     *       console.log(`Deleted path ${i++}: ${deletedPath.name}, deleted on: ${deletedPath.deletedOn}`);
     *     }
     *   }
     * }
     * ```
     *
     * Example using paging with a marker:
     *
     * ```ts snippet:ReadmeSampleListDeletedPaths_Continuation
     * import { DataLakeServiceClient } from "@azure/storage-file-datalake";
     * import { DefaultAzureCredential } from "@azure/identity";
     *
     * const account = "<account>";
     * const datalakeServiceClient = new DataLakeServiceClient(
     *   `https://${account}.dfs.core.windows.net`,
     *   new DefaultAzureCredential(),
     * );
     *
     * const fileSystemName = "<file system name>";
     * const fileSystemClient = datalakeServiceClient.getFileSystemClient(fileSystemName);
     *
     * let i = 1;
     * let deletedPaths = fileSystemClient.listDeletedPaths().byPage({ maxPageSize: 2 });
     * let response = (await deletedPaths.next()).value;
     * // Prints 2 deleted paths
     * if (response.deletedPathItems) {
     *   for (const deletedPath of response.deletedPathItems) {
     *     console.log(`Deleted path ${i++}: ${deletedPath.name}, deleted on: ${deletedPath.deletedOn}`);
     *   }
     * }
     * // Gets next marker
     * let marker = response.continuationToken;
     * // Passing next marker as continuationToken
     * deletedPaths = fileSystemClient
     *   .listDeletedPaths()
     *   .byPage({ continuationToken: marker, maxPageSize: 10 });
     * response = (await deletedPaths.next()).value;
     * // Prints 10 deleted paths
     * if (response.deletedPathItems) {
     *   for (const deletedPath of response.deletedPathItems) {
     *     console.log(`Deleted path ${i++}: ${deletedPath.name}, deleted on: ${deletedPath.deletedOn}`);
     *   }
     * }
     * ```
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/list-blobs
     *
     * @param options - Optional. Options when listing deleted paths.
     */
    listDeletedPaths(options?: ListDeletedPathsOptions): PagedAsyncIterableIterator<DeletedPath, FileSystemListDeletedPathsResponse>;
    private listDeletedItems;
    private listDeletedSegments;
    private listDeletedPathsSegment;
    /**
     * Restores a soft deleted path.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/undelete-blob
     *
     * @param deletedPath - Required.  The path of the deleted path.
     *
     * @param deletionId - Required.  The deletion ID associated with the soft deleted path.
     *
     */
    undeletePath(deletedPath: string, deletionId: string, options?: FileSystemUndeletePathOption): Promise<FileSystemUndeletePathResponse>;
    /**
     * Only available for DataLakeFileSystemClient constructed with a shared key credential.
     *
     * Generates a Service Shared Access Signature (SAS) URI based on the client properties
     * and parameters passed in. The SAS is signed by the shared key credential of the client.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/constructing-a-service-sas
     *
     * @param options - Optional parameters.
     * @returns The SAS URI consisting of the URI to the resource represented by this client, followed by the generated SAS token.
     */
    generateSasUrl(options: FileSystemGenerateSasUrlOptions): Promise<string>;
    /**
     * Only available for DataLakeFileSystemClient constructed with a shared key credential.
     *
     * Generates string to sign for a Service Shared Access Signature (SAS) URI based on the client properties
     * and parameters passed in. The SAS is signed by the shared key credential of the client.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/constructing-a-service-sas
     *
     * @param options - Optional parameters.
     * @returns The SAS URI consisting of the URI to the resource represented by this client, followed by the generated SAS token.
     */
    generateSasStringToSign(options: FileSystemGenerateSasUrlOptions): string;
    /**
     * Generates a Service Shared Access Signature (SAS) URI based on the client properties
     * and parameters passed in. The SAS is signed by the input user delegation key.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/constructing-a-service-sas
     *
     * @param options - Optional parameters.
     * @param userDelegationKey - Return value of `blobServiceClient.getUserDelegationKey()`
     * @returns The SAS URI consisting of the URI to the resource represented by this client, followed by the generated SAS token.
     */
    generateUserDelegationSasUrl(options: FileSystemGenerateSasUrlOptions, userDelegationKey: UserDelegationKey): Promise<string>;
    /**
     * Generates string to sign for a Service Shared Access Signature (SAS) URI based on the client properties
     * and parameters passed in. The SAS is signed by the input user delegation key.
     *
     * @see https://learn.microsoft.com/rest/api/storageservices/constructing-a-service-sas
     *
     * @param options - Optional parameters.
     * @param userDelegationKey - Return value of `blobServiceClient.getUserDelegationKey()`
     * @returns The SAS URI consisting of the URI to the resource represented by this client, followed by the generated SAS token.
     */
    generateUserDelegationSasStringToSign(options: FileSystemGenerateSasUrlOptions, userDelegationKey: UserDelegationKey): string;
}
//# sourceMappingURL=DataLakeFileSystemClient.d.ts.map