{"version":3,"file":"PathClientInternal.js","sourceRoot":"","sources":["../../../src/utils/PathClientInternal.ts"],"names":[],"mappings":";;;AAAA,uCAAuC;AACvC,kCAAkC;AAClC,8CAAmD;AACnD,mEAAkF;AAGlF;;;GAGG;AACH,MAAa,kBAAmB,SAAQ,+BAAkB;IACxD;;OAEG;IACI,eAAe,CAAO;IAE7B;;;;;;;;OAQG;IACH,YAAmB,GAAW,EAAE,QAAkB;QAChD,KAAK,CAAC,GAAG,EAAE,QAAQ,CAAC,CAAC;QACrB,IAAI,CAAC,eAAe,GAAG,IAAI,6BAAI,CAAC,IAAI,CAAC,kCAAkC,CAAC,CAAC;IAC3E,CAAC;CACF;AAnBD,gDAmBC","sourcesContent":["// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT License.\nimport { DataLakePathClient } from \"../clients.js\";\nimport { PathOperationsImpl as Path } from \"../generated/src/operations/index.js\";\nimport type { Pipeline } from \"@azure/storage-blob\";\n\n/**\n * A PathClientInternal represents a URL to the Azure Storage path (directory or file) to\n * help to construct a path client to expose Path context with blob endpoint.\n */\nexport class PathClientInternal extends DataLakePathClient {\n  /**\n   * Path context with blob endpoint.\n   */\n  public blobPathContext: Path;\n\n  /**\n   * Creates an instance of DataLakePathClient from url and pipeline.\n   *\n   * @param url - A Client string pointing to Azure Storage data lake path (directory or file), such as\n   *                     \"https://myaccount.dfs.core.windows.net/filesystem/directory\" or \"https://myaccount.dfs.core.windows.net/filesystem/file\".\n   *                     You can append a SAS if using AnonymousCredential, such as \"https://myaccount.dfs.core.windows.net/filesystem/directory?sasString\".\n   * @param pipeline - Call newPipeline() to create a default\n   *                            pipeline, or provide a customized pipeline.\n   */\n  public constructor(url: string, pipeline: Pipeline) {\n    super(url, pipeline);\n    this.blobPathContext = new Path(this.storageClientContextToBlobEndpoint);\n  }\n}\n"]}